# Local Development Configuration

spotify.pipeline {
  
  # Database configurations (Local PostgreSQL)
  database {
    raw {
      url = "jdbc:postgresql://"${DB_HOST}":"${DB_PORT}"/"${DB_NAME}
      user = ${DB_USER}
      password = ${DB_PASSWORD}
      driver = "org.postgresql.Driver"
      connection_pool_size = 5
    }
    
    # For local development, use the same PostgreSQL instance
    analytics {
      url = "jdbc:postgresql://"${DB_HOST}":"${DB_PORT}"/"${DB_NAME}
      user = ${DB_USER}
      password = ${DB_PASSWORD}
      driver = "org.postgresql.Driver"
      connection_pool_size = 3
    }
  }
  
  # Local file system storage instead of Azure Data Lake
  local {
    storage {
      base_path = ${DATA_PATH}"/processed"
      raw_path = ${RAW_DATA_PATH}
      processed_path = ${PROCESSED_DATA_PATH}
      analytics_path = ${ANALYTICS_DATA_PATH}
      
      # File formats
      format = "parquet"
      compression = "snappy"
    }
  }
  
  # Apache Beam configuration for local development
  beam {
    runner = "DirectRunner"
    project = "spotify-local-pipeline"
    temp_location = ${DATA_PATH}"/tmp"
    
    # Local runner options
    direct_runner {
      num_threads = 4
      target_parallelism = 2
    }
  }
  
  # Streaming history processing configuration
  streaming_history {
    # SQL query to extract raw streaming events
    sql_query = """
      SELECT 
        CONCAT(user_id, '_', track_id, '_', EXTRACT(EPOCH FROM played_at)) as event_id,
        user_id,
        track_id,
        played_at as timestamp,
        ms_played,
        reason_start,
        reason_end,
        skipped,
        shuffle,
        platform,
        u.country
      FROM raw.streaming_history sh
      JOIN raw.users u ON sh.user_id = u.user_id
      WHERE DATE(played_at) = ?
      ORDER BY played_at
    """
    
    # Output configuration for local development
    output {
      # Local file system output
      parquet {
        path = ${spotify.pipeline.local.storage.processed_path}"/streaming_events/"
        compression = "snappy"
        partition_columns = ["date", "country"]
      }
      
      # Local PostgreSQL analytics tables
      analytics {
        schema = "analytics"
        table_user_stats = "daily_user_listening_stats"
        table_track_popularity = "track_popularity_metrics"
      }
    }
  }
  
  # Data quality and monitoring
  quality {
    enable_validation = true
    output_metrics = true
    metrics_sink = "local_file"
    metrics_path = ${DATA_PATH}"/metrics/"
  }
  
  # Kafka configuration for local development
  kafka {
    bootstrap_servers = ${KAFKA_BOOTSTRAP_SERVERS}
    topics {
      streaming_events = ${KAFKA_TOPIC_STREAMING}
    }
    consumer {
      group_id = "spotify-local-consumer"
      auto_offset_reset = "earliest"
    }
  }
}