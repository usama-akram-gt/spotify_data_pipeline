# Azure-based Spotify Pipeline Configuration

spotify.pipeline {
  
  # Database configurations
  database {
    raw {
      url = "jdbc:postgresql://"${DB_HOST}":"${DB_PORT}"/"${DB_NAME}
      user = ${DB_USER}
      password = ${DB_PASSWORD}
      driver = "org.postgresql.Driver"
      connection_pool_size = 10
    }
    
    # Databricks as data warehouse
    analytics {
      url = "jdbc:databricks://"${DATABRICKS_WORKSPACE_URL}":443/default;transportMode=http;ssl=1;AuthMech=3;httpPath=/sql/1.0/warehouses/"${DATABRICKS_WAREHOUSE_ID}";ConnCatalog=hive_metastore"
      user = "token"
      password = ${DATABRICKS_ACCESS_TOKEN}
      driver = "com.databricks.client.jdbc.Driver"
      connection_pool_size = 5
    }
  }
  
  # Azure Data Lake Storage Gen2 configuration
  azure {
    data_lake {
      account_name = ${ADLS_ACCOUNT_NAME}
      account_key = ${ADLS_ACCOUNT_KEY}
      container_name = ${ADLS_CONTAINER_NAME}
      
      # Authentication
      auth {
        type = "account_key" # or "service_principal"
        client_id = ${?AZURE_CLIENT_ID}
        client_secret = ${?AZURE_CLIENT_SECRET}
        tenant_id = ${?AZURE_TENANT_ID}
      }
      
      # File paths
      paths {
        raw = "raw"
        bronze = "bronze"
        silver = "silver"
        gold = "gold"
        checkpoints = "checkpoints"
      }
      
      # Base URL
      base_url = "abfss://"${ADLS_CONTAINER_NAME}"@"${ADLS_ACCOUNT_NAME}".dfs.core.windows.net/"
    }
  }
  
  # Beam pipeline configuration
  beam {
    runner = "DirectRunner" # Change to DataflowRunner for production
    project = "spotify-pipeline"
    temp_location = ${spotify.pipeline.azure.data_lake.base_url}"tmp/"
    staging_location = ${spotify.pipeline.azure.data_lake.base_url}"staging/"
    
    # Dataflow specific (if using Google Cloud)
    dataflow {
      region = "us-central1"
      machine_type = "n1-standard-2"
      num_workers = 2
      max_num_workers = 10
    }
  }
  
  # Streaming history processing configuration
  streaming_history {
    # SQL query to extract raw streaming events
    sql_query = """
      SELECT 
        CONCAT(user_id, '_', track_id, '_', EXTRACT(EPOCH FROM played_at)) as event_id,
        user_id,
        track_id,
        played_at as timestamp,
        ms_played,
        reason_start,
        reason_end,
        skipped,
        shuffle,
        platform,
        u.country
      FROM raw.streaming_history sh
      JOIN raw.users u ON sh.user_id = u.user_id
      WHERE DATE(played_at) = ?
    """
    
    # Output configuration
    output {
      # Parquet output to Data Lake
      parquet {
        path = ${spotify.pipeline.azure.data_lake.base_url}${spotify.pipeline.azure.data_lake.paths.silver}"/streaming_events/"
        compression = "snappy"
        partition_columns = ["date", "country"]
      }
      
      # Analytics output to Databricks
      databricks {
        catalog = "hive_metastore"
        schema = "analytics"
        table_user_stats = "daily_user_listening_stats"
        table_track_popularity = "track_popularity_metrics"
      }
    }
  }
  
  # Data quality and monitoring
  quality {
    enable_validation = true
    output_metrics = true
    metrics_sink = "databricks"
  }
}

# Hadoop configuration for Azure authentication
hadoop {
  fs.azure.account.key.${ADLS_ACCOUNT_NAME}.dfs.core.windows.net = ${ADLS_ACCOUNT_KEY}
  fs.azure.account.auth.type.${ADLS_ACCOUNT_NAME}.dfs.core.windows.net = "SharedKey"
}